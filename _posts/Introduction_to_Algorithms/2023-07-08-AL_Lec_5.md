---
layout: article
title: Linear-time Sorting:Lower Bounds, Counting Sort, Radix Sort
tags: Introduction_to_Algorithms
aside:
  toc: true
sidebar:
  nav: Introduction_to_Algorithms
---

In this lecture, we still talk about sorting and we want to ask <b>"How fast can we sort?"</b>. Let's review the running time of previous learned Algorithms, the Heapsort and Merge Sort can achieve ${ O(n \lg n) }$ in the worst case. Randomized Quicksort can run in ${ O(n \lg n) }$ time on average. And Insetion Sort is ${ O(n^2) }$. All these Algortihms run no faster than ${ O(n \lg n) }$. 

<b>So can we do better?</b>

<!--more-->

<b>No but Yes!</b>. In fact, it depends on what kinds of manipulation are allowed. The Algorithms mentioned before have a common property: <i>the sorted order they determine is based only on <b>comparisons</b> between the input elements</i>. So, such sorting algorithms are called <b>comparison sort</b>. In this sense, the Anwser is <b>No</b>! We gonna prove that any comparison sort cannot run faster than ${ O(n \lg n) }$ in the worst case. 

From another way, if we use other oprations more than "<b>Comparison</b>", we can get some sorting algorithms faster than ${ O(n \lg n) }$. In this lecture, we will introduce <b>Counting sort</b> and <b>Radix sort</b>.

# Lower bounds for Comparison sorts

In a compasison sort, we can only use comparisons to gain the order information about input sequence ${ \left< a_1,a_2,\cdots, a_n \right> }$. In this section, we assume the all elements in array are distinct. Here we introduce <b>decision tree</b>. 

## Decision tree model

We can view the comparison sorts in terms of decision tree. A <b><i>decision tree</i></b> is a full binary tree that represents the comparsion between elements that are performed by a particular sorting algorithm operating on an input of a given size. In another words, for a given size of input, we can use decision tree to represent the algorithm, in which we only record each comparison in node, and the different result will lead to different child nodes. 

Let's take an example to get some intuition. Suppose we want to sort three elements ${ \left< a_1, a_2, a_3\right> }$. Here is the solution of insertion sort

<q>first move to second element ${ a_2 }$, and then <b>compare</b> ${a_2}$ to ${ a_1 }$. If ${ a_2 \geq a_1 }$, then we move to ${ a_3 }$, <b>compare</b> ${ a_2, a_3 }$ ... consider all the situation, we can get following decision tree</q>

<p align="center">
    <img src="/post_image/Introduction_to_Algorithm/Lec_5
/Decision_tree_three.png" width="80%">
</p>

__The decision tree for insertion sort operating on three elements.__

### Definition of Decision tree

In general, for a given list ${ \left< a_1,a_2,\cdots, a_n \right> }$

* Each internal node (non-leaf node) has a lable "${ i:j }$", ${ i, j \in \{ 1,2,\cdots, n \} }$, which means we compare ${ a_i , a_j }$

* Left subtree gives the subsequent comparisons if ${ a_i \leq a_j }$

* Right subtree gives the subsequent comparisons if ${ a_i > a_j }$

* Each leaf node gives a permutation ${ \left< \pi(1), \pi(2), \cdots, \pi(n) \right> }$ such that ${  a_{\pi(1)} \leq a_{\pi(2)} < \cdots <  a_{\pi(n)} }$

### Decision tree model comparison sorts

* One tree for each input size ${ n }$

* View algorithms as splitting whenever it makes a comparision. 

* Tree lists comparisons along all posible instruction traces.

The number of leaves is ${ n! }$, which is all the possible permutation of ${ n }$ elements.

## Lower bounds

* The runing time of one certain case raltes to the number of comparision, which equals to the length of the path from root to the leaf.

* The worst-case runing time is the height of the tree.

<b>Theorem</b> Any comparison sort algorithm requires ${ \Omega{n \lg n}  }$ lg n/ comparisons in the worst case.

Proof. The number of leaves is at least ${ n! }$. We denote the tree of the tree as ${ h }$. So, we have to guarantee

<center>$$
\begin{equation}
\begin{aligned}
2^h &\geq n! \\
h &\geq \lg (n!) &\text{(since the lg function is monotonically increasing)} \\
h & = \Omega(n \lg n) &\text{(Stirling's formula)} \\
\end{aligned}
\end{equation}
$$</center>

<b>Corollary</b> Heapsort and merge sort are asymptotically optimal comparison sorts.

Proof. It's easy to check from above Theorem.

### Randomized algorithm

The above conclusions apply to "Deterministic algorithms" (like Heapsort, Insertion sort). What a Deterministic algorithm does is completely determine at each step! But, for a randomized algorithm, it will depend on some Randomized factors. Reviewing the ["Definition of Decision Tree"](#definition-of-decision-tree), we assume only one tree for each input size ${ n }$. Therefore, for randomized algortihms, we actually get a series of trees (the probability distribution of trees). But the lower bound is still applies to it, though. Because, no matter what tree we get, the above conclusion applies to every tree. 

# Counting sort

<b>Counting sort</b> assumes that each of the ${ n }$ input elements ${ A[i] \in \{0,2, \cdots , k \} }$. The idea of <b>Counting sort</b> is counting how many elements less than each element ${ x }$, and determining the postion of ${ x }$. Like there are ${ 17 }$ elements less than ${ x }$, ${ x }$ will be put on the ${ 18^{\text{th}} }$ position in output array. 

In the pseudocode of Counting sort, we use ${ A[1..n] }$ to represent input array, ${ B[1..n] }$ to denote output array, and ${ C[1..k] }$ as temporary auxiliary storage.

{% highlight pseudocode linenos %}
COUNTING-SORT(A,B,K)
let C[0..k] be a new array
for i = 0 to k
    C[i] = 0
for j = 1 to n
    C[A[j]] =  C[A[j]] + 1 // C[i] now counts the number i in array A.
for i = 1 to k
    C[i] = C[i] + C[i-1] // C[i] now stores the number of elements which are not greater than i. 
for j = n downto 1
    B[C[A[j]]] = A[j] //Input A[j] to B, its position is determined by C[A[j]]
    C[A[j]] = C[A[j]] - 1 // Because element A[j] has been put into B, and the conuting number minus 1.
{% endhighlight %}

For second loop, we use the ${ i^{\text{th}} }$ postion of ${ C }$ to count the number of elements which equals to ${ i }$, that is ${ C[i] = \vert \{e = i, e  \in A[1..n]\}\vert }$. For third loop, array ${ C[i] }$ holds the number less than or equal to ${ i }$, ${ C[i] = \vert \{e \leq  i, e  \in A[1..n]\}\vert   }$ which also represents the last position of element that equals to ${ i }$. The last loop puts all the elements of ${ A }$ to correct position. We iterate ${ A }$ from the end, each time when we pick up ${ A[j] }$, we can get the position from ${ C[A[j]] }$ and update ${ C[i] }$. 

From the pseudocode, we can easily get the runing time of Algorithm is ${ \Theta(k+n) }$. If ${ k = O(n) }$, we will get a linear algorithm. 

## Stability of Sorting Algorithm

An important property of counting sort is that it is <b>stable</b>: numbers with the same value appear in the <b>output array</b> in the <b>same order</b> as they do in the <b>input array</b>.


# Radix sort