---
layout: article
title: Exercises in Lecture 2
tags: Introduction_to_Algorithms
aside:
  toc: true
sidebar:
  nav: Introduction_to_Algorithms
---

# Polynomially bounded

Is the function ${ \lceil \lg n \rceil  ! }$ polynomially bounded? Is the function ${ \lceil \lg \lg n \rceil !}$ polynomially bounded?

Proving that a funtion ${ f(n) }$ is polynomially bounded is equivalent to proving that ${ \lg (f(n)) = O(\lg n)}$. 

<!--more-->

That's because:

${\Rightarrow}$ if ${ f(n) }$ is polynomially bounded means there exists constants ${ c,k,n_0 }$, when ${ n>n_0 }$, ${ f(n)\leq cn^k }$. Hence, ${ \lg (f(n)) \leq k \lg n + \lg c \leq (k+1) \lg n }$, that means ${ \lg (f(n)) = O(\lg n) }$.

${\Leftarrow}$ if ${ \lg (f(n)) = O(\lg n) }$, that means, there exists constants ${ k,n_0 }$ such that ${ \lg (f(n)) \leq k \lg n }$, when ${ n>n_0 }$. Therefore, ${ f(n) \leq n^k }$. So, the 
above proposition is true.

In the following, we will use two fact

\#1. ${\lg (n!) = \Theta(n\lg n)}$ (We can check it by [stirling's approximation](https://en.wikipedia.org/wiki/Stirling%27s_approximation)).

\#2. ${\lceil \lg n \rceil = \Theta(\lg n)}$ (Because ${ \lg n \leq \lceil \lg n \rceil \leq \lg n +1 }$).

Now, we can check the ${ \lg (\lceil \lg n \rceil  ! )}$ 

<center>$$
\begin{equation}
\begin{aligned}
\lg (\lceil \lg n \rceil  ! ) &= \Theta(\lceil \lg n\rceil \lg (\lceil \lg n \rceil)) \\
& = \Theta(\lg n \cdot \lg \lg n) \\
& \neq O(\lg n)
\end{aligned}
\end{equation}
$$</center>

and ${ \lg (\lceil \lg \lg n \rceil  ! )}$

<center>$$
\begin{equation}
\begin{aligned}
\lg (\lceil \lg \lg n \rceil  ! ) &= \Theta(\lceil \lg \lg n \rceil \cdot \lg \lceil \lg \lg n \rceil) \\
&= \Theta(\lg \lg n \cdot \lg \lg \lg n) \\
&= o(\lg \lg n \cdot \lg \lg n) \\
&= o(\sqrt {\lg n} \cdot \sqrt {\lg n} ) \\
&= O(\lg n)
\end{aligned}
\end{equation}
$$</center>

# Asymptotic Notation Exercise

(a) ${ f(n)=O((f(n)^2)) }$

<b>Sometimes true</b>, when ${ f(n)=n }$, that's true. But when ${ f(n) = 1/n }$, that's not true!

(b) ${ f(n) + g(n) = \Theta\left(\max\left(f(n),g(n)\right)\right) }$

<b>Always true</b>. That's because

<center>$$
\max(f(n),g(n)) \leq f(n) + g(n) \leq 2\cdot \max(f(n),g(n))
$$</center>

(c) ${ f(n)+O(f(n)) =\Theta(f(n)) }$

<b>Always true</b>. Because for any ${ g(n) \in O(f(n)) }$, there exists ${ c,n_0 }$ such that ${ g(n) \leq c f(n) }$, when ${ n>n_0 }$. Therefore

<center>$$
f(n) \leq f(n) + g(n) \leq (c+1) f(n)
$$</center>

So, for any ${ g(n) \in O(f(n)) }$, ${ f(n) + g(n) = \Theta (f(n)) }$, that means ${ f(n)+O(f(n))=\Theta(f(n)) }$.

(d) ${ f(n) = \Omega(g(n)) }$ and ${ f(n) = o(g(n)) }$

<b>Never true</b>. ${ f(n) = \Omega(g(n)) }$ means ${ \exists \varepsilon, n_0 }$, such that ${ g(n) \leq \varepsilon \cdot f(n) }$, when ${ n>n_1 }$. And ${ f(n) = o(g(n)) }$ means ${ \forall c >0, \exists n_c }$ such that ${ f(n) \leq c g(n) }$, when ${ n>n_c }$.

Therefore, for ${ c=\frac{1}{2\varepsilon}  }$, when ${ n > \max (n_c, n_0) }$, we have

<center>$$
f(n) \leq \frac{1}{2\varepsilon} \cdot g(n) \leq \varepsilon \frac{1}{2\varepsilon} \cdot f(n)
$$</center>

That's cannot true!

(e) ${ f(n)\neq O(g(n)) }$ and ${ g(n)\neq O(f(n)) }$

<b>Sometimes true</b>. Consider ${ f(n) = 1, g(n) = \lVert n \sin n \rVert}$. Actually, ${ f(n) \neq O( g(n)) }$ means, for any ${ c,n_0 }$, there always exists some ${ n>n_0 }$, we have ${g(n) \geq cf(n) }$

# Solve Recurrences

Give asymptotic upper and lower bounds for ${ T(n) }$ in each of the following recurrences. Assume that ${ T(n) }$ is constant for ${ n\leq 10 }$. Make your bounds as tight as possible, and justify it.

(a) ${ T(n) = 2T(n/3)+ n\lg n}$

We will use Master Method to solve it. In this case ${ a=2,b=3,f(n)=n \lg n }$, let's compare  ${ n^{\log_3 2} }$ and ${ f(n)=n \lg n }$

<center>$$
\begin{equation}
\begin{aligned}
n^{\log_3 2} &= n^{\log_3 3\cdot \frac{2}{3} }\\
&= n^{1-\log_3 \frac{3}{2}}
\end{aligned}
\end{equation}
$$</center>

Let ${ \varepsilon = \log_3 \frac{3}{2} >0 }$, we have 

<center>$$
\begin{equation}
\begin{aligned}
f(n) &= n \lg n \\
& = \Omega(n) \\
&= \Omega(n^{\log_3 2 +\varepsilon})
\end{aligned}
\end{equation}
$$</center>

Besides, ${ af(n/b)= \frac{an}{b} \lg \frac{n}{b} \leq \frac{an}{b} \lg n= \frac{2}{3} f(n)}$. So, we are in Case \#3 of Master Theorem, that is ${ T(n)= \Theta(f(n))=\Theta(n \lg n)}$.

(b) ${ T(n)=3T(n/5) + \lg^2 n }$

In this case, ${ a=3,b=5,f(n)=\lg^2 n }$. We will use following fact
<details><summary>
${\lg n = O\left(n^{\frac{1}{3}}\right)}$</summary>

Let ${ g(n)= \frac{3}{\ln 2} \cdot n^{\frac{1}{3}} - \lg n }$, then 

<center>$$
g'(n) = \frac{1}{\ln 2}\cdot (n^{-\frac{2}{3}} - n^{-1})
$$</center>

It's easy to check, ${ g'(n) <0 }$ when ${ n<1 }$, and ${g'(n) \geq 0 }$ when ${ n \geq 1 }$.

So, ${ g(n) \geq g(1)=\frac{3}{\ln 2} >0 }$, that means ${ \lg n = O (n^{\frac{1}{3}} ) }$.

</details>

Hence, we can compare ${ f(n) }$ and ${ n^{\log_5 3} }$, let ${ \varepsilon = \log_5 3 -  \frac{2}{3}=\log_5 \frac{3}{\sqrt[3] {5^2}} = \log_5 \frac{\sqrt[3]{27}}{\sqrt[3] {25}} >0}$

<center>$$
\begin{equation}
\begin{aligned}
f(n) &= \lg^2 n \\
&= O(n^{\frac{2}{3}} ) \\
&= O(n^{\log_5 3 - \varepsilon})
\end{aligned}
\end{equation}
$$</center>

According to Mater Theorem Case \#1, ${ T(n)=\Theta(n^{\log_5 3}) }$.

(c) ${ T(n) = T(n/2) + 2^n }$

Here ${ a=1,b=2, f(n)=2^n }$, it's trivial to check ${ f(n)=2^n = \Omega(n^{\log_2 1 + \varepsilon})=\Omega(n^{ \varepsilon}) }$, for example ${ \varepsilon = 1 }$ can make it right.

Then, when ${ n > 2 }$, it's easy to check 

<center>$$
f(n/2) = 2^{n/2} < \frac{1}{2} \cdot 2^n =\frac{1}{2} f(n)
$$</center>

So, we are in the Case \#3 of Master Theorem. And we can get ${ T(n)= \Theta(f(n))=\Theta(2^n) }$.

(d)