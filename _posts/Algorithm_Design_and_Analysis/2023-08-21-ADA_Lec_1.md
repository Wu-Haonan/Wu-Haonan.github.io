---
layout: article
title: Dynamic Programming, Weighted interval Scheduling
tags: Algorithm_Design_and_Analysis
aside:
  toc: true
sidebar:
  nav: Algorithm_Design_and_Analysis
---

This blog I will share my notes in Penn State CSE 565: Algorithm Design and Analysis. 

<!--more-->

# Dynamic Program

<b>key point</b>: divide into small instances

1. Recursivly divide the main problem into sub problem which could be <b>overlapping</b>.

2. start by solving the subproblems and then combind them

ex. max subarray problem

3. In order to get an optimal solution for the problem, we first need to find the optimal solution for the subproblem. Then combine these solution of the smaller subprblems to find a solution of the larger ones.

4. However, unlike greedy in the DP, combing the subproblems are much more non-trivial. 


Problems

1. Weighted Interval scheduling

2. Knapsack problem

3. Fibonicci number

4. RNA folding

5. sequence alignment 
    * Edit distance (ED) 
    * Longest common sequence (LCS) 

## Weighted interval Scheduling

we are given a set of jobs ${ 1 \cdots n }$

1. Here job j starts at time stamp ${ s_j }$
    finishes at ${ f_j }$, and has a weight ${ w_j }$.

2. Two jobs i,j are compatible, if they do not overlap, that is either

<center>$$
s_j > f_i \text{ or } s_i > f_j$$</center>

* Objective: find a subset of mutually compatible jobs that maximizes the weight.

### Unweighted Interval Scheduling. 

Let's consider the easy version. We are going to ignore the weights.  

For each ${ w_j  = 1}$. Thus the objective is to maximize the size of the set of mutually compatible jobs. We will use a greedy algorithm

#### Algorithm

1. Consider the jobs in increasing order of their finish time ${ f_1 \leq f_2 \cdots \leq f_n }$

2. At the ${ i^th }$ round, consider job i, and if it's compatible with all the previously selected jobs, add i

#### Correctness

Let's prove its correctness.

Proof. Let's denote the set got by greedy algorithm as ${ I =\{ i_1,i_2,\cdots,i_k \}}$. And the optimal solution as ${ J= \{ j_1,\cdots,j_m \} }$.

Assume ${ i_1,i_2,\cdots,i_k }$ is not the optimal set. So ${ m > k  }$

For ${ q = 1,2,\cdots, k }$, compare ${ i_q \text{ and } j_q }$. If ${ i_q = j_q }$, move to next comparison. If not, we can replace ${ j_q }$ as ${ i_q }$. Because ${ \forall p< q, i_p = j_p }$ and we sort finish time at first, that is,${ f_{i_q} < f_{j_q}}$, this operation still keep solution ${ J }$ optimal.

When ${ q = k+1 }$, we cannot find another feasible job, otherwise, we can extend solution ${ I }$. Therefore, ${ m = k }$, which contradicts to ${ m>k }$. Thus, solution ${ I }$ is optimal.

Does it work? No!

### DP Algorithm

Notation

1. order the jos in increasing order of finish time.

2. For a job j, define

P(j) = largest index i < j, such that i,j are non-overlapping.

DP Formulation

OPT(j) = value of the optimal solution of {1,2,...,j}

OPT(j) = 0 if j = 0

Case 1: OPT(j) includes job j 
-> Add w_j to OPT
-> all jobs from {P(j)+1, ..., j-1} are incompatible with j
-> Find OPT(P(j))

Case 2: OPT(j) does not include job
-> Find OPT(j-1)

<center>$$
OPT(j) = 
\begin{cases}
0 \quad if \quad j = 1\\
max \{w_j OPT(P(j)), OPT(j-1) \}, \quad otherwise
\end{cases}
$$</center>

3. Correctness

Theorem:

Compute

Proof: Base case j = 0, OPT(0) = 0

Induction step

1,2,...,j

Let for all i <j, computer OPT(i) = OPT(i)

* As P(j) < j, computer OPT(P(j)) = OPT(P(j))

* As j-1 < j, compute OPT(j-1) = OPT(j-1)

Now we argue for j

Compute OPT(j) = max {} = OPT(j)













