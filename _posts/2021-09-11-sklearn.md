---
layout: article
title: 机器学习 Sklearn
tags: Python
---
主要参考b站视频:[【2020机器学习全集】菜菜的sklearn完整版](https://www.bilibili.com/video/BV1MA411J7wm?p=4&spm_id_from=pageDriver)

<!--more-->

<b>Sklearn基本建模流程：</b>

{% highlight python linenos %}
from sklearn import tree              #导入需要的模块
clf = tree.DecisionTreeClassifier()   #实例化
clf = clf.fit(X_train,y_train)        #用训练集训练模型
result = clf.score(X_test,y_test)     #导入测试集，从接口中调用需要的信息
{% endhighlight %}

# 决策树
决策树常用模块：

{% highlight python linenos %}
tree.DecisionTreeClassifier  #分类树
tree.DecisionTreeRegressor   #回归树
tree.export_graphviz         #将生成的决策树导出为DOT格式，画图专用
tree.ExtraTreeClassifier     #高随机版本的分类树
tree.ExtraTreeRegressor      #高随机版本的回归树
{% endhighlight %}

## Sklearn重要参数
### criterion
一般用基尼系数，拟合程度不够用信息熵

criterion 这个参数是用来决定不纯度的计算方法。Sklearn 提供了两种选择：

1.输入"entropy"，使用信息熵（Entropy）

2.输入"gini"，使用基尼系数（Gini Impurity）

比起基尼系数,信息熵对不纯度更加敏感，对不纯度的惩罚最强.但是在实际使用中，信息熵和基尼系数的效果基本相同.但是信息熵的计算比基尼系数缓慢一些，因为基尼系数的计算不涉及对数.另外，因为信息熵对不纯度更加敏感，所以信息熵作为指标时，决策树的生长会更加"精细",因此对于高纬数据或者噪声很多的数据，信息熵很容易过拟合，基尼系数在这种情况下效果往往比较.当模型拟合程度不足时，即当模型在训练集和测试集上都表现不太好的时候，使用信息熵.当然，这些不是绝对的.

{% highlight python linenos %}
from sklearn import tree
from sklearn.datasets import load_wine      # 载入scikit-learn 内置的红酒数据集
from sklearn.model_selection import train_test_split    #数据集拆分工具
wine = load_wine()
print(wine.feature_names)
print(wine.target_names)
Xtrain,Xtest,Ytrain,Ytext = train_test_split(wine.data,wine.target,test_size=0.3)   #拆分训练集和测试集，test_size=0.3表示数据集的百分之三十拿来做测试集
print(Xtrain.shape)
clf = tree.DecisionTreeClassifier(criterion='entropy')
clf = clf.fit(Xtrain,Ytrain)
score = clf.score(Xtest,Ytext)  #返回预测的准确度
print(score)
{% endhighlight %}

### 如何画出决策树以及判断特征重要性
{% highlight python linenos %}
from sklearn import tree
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
wine = load_wine()
Xtrain,Xtest,Ytrain,Ytest = train_test_split(wine.data,wine.target,test_size=0.25)
clf = tree.DecisionTreeClassifier(criterion='entropy')
clf = clf.fit(Xtrain,Ytrain)
score = clf.score(Xtest,Ytest)
print(score)
import graphviz
dot_data = tree.export_graphviz(clf
                                ,feature_names = wine.feature_names
                                ,class_names = ['0','1','2']
                                ,filled = True      # filled = Ture是给决策树添加颜色，颜色越深不纯度越低；
                                ,rounded = True)    # rounded = Ture是让决策树的方框变圆；每一行逗号写在前面，是为了看这个参数有什么效果时候，方便把其参数注释掉
graph = graphviz.Source(dot_data)
graph.view()
print(clf.feature_importances_)     # 判断决策树的特征重要性
print([*zip(wine.feature_names,clf.feature_importances_)])      #将特征和特征重要性放在一起，方便观察
{% endhighlight %}

### random_state或者splitter

random_state用来设置分支中的随机模式的参数；splitter也是用来控制决策树中的随机选项的，有两种输入值，输入‘best’决策树在分支时候虽然随机但是会优先选择更重要的特征进行分支，重要性也是通过feature_importances_查看；输入'random'决策树分支会更加随机，对训练集的拟合将会降低，可以防止过拟合。

{% highlight python linenos %}
clf = tree.DecisionTreeClassifier(criterion='entropy'
                                  ,random_state=30
                                  ,splitter='random')
clf = clf.fit(Xtrain,Ytrain)
score = clf.score(Xtest,Ytest)
print(score)
import graphviz
dot_data = tree.export_graphviz(clf
                                ,feature_names = wine.feature_names
                                ,class_names=['0','1','2']
                                ,filled=True
                                ,rounded=True)
graph = graphviz.Source(dot_data)
print(graph)
graph.view()
{% endhighlight %}