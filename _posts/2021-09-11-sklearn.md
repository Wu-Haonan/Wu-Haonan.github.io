---
layout: article
title: 机器学习 Sklearn
tags: Python
---
主要参考b站视频:[【2020机器学习全集】菜菜的sklearn完整版](https://www.bilibili.com/video/BV1MA411J7wm?p=4&spm_id_from=pageDriver)

主要内容：

<!--more-->

<b>Sklearn基本建模流程：</b>
{% highlight python linenos %}
from sklearn import tree              #导入需要的模块
clf = tree.DecisionTreeClassifier()   #实例化
clf = clf.fit(X_train,y_train)        #用训练集训练模型
result = clf.score(X_test,y_test)     #导入测试集，从接口中调用需要的信息
{% endhighlight %}

# 决策树

## 1.分类树
分类树DecisionTreeClassifier和决策树绘图(export_graphviz)的所有基础知识;决策树的基本流程(找特征，计算不纯度，分类);分类树的八个参数，一个属性，四个接口，以及绘图所用的代码.

八个参数：Criterion,两个随机性相关的参数（random_state,splitter）,
五个剪枝参数（max_depth,min_samples_split,min_samples_leaf,max_feature,min_impurity_decrease）
一个属性：feature_importances_
四个接口：fit,score,apply,predict

### 1.1 Sklearn重要参数

#### 1.1.1 criterion

一般用基尼系数，拟合程度不够用信息熵

criterion 这个参数是用来决定不纯度的计算方法。Sklearn 提供了两种选择：

1.输入"entropy"，使用信息熵（Entropy）

2.输入"gini"，使用基尼系数（Gini Impurity）

比起基尼系数,信息熵对不纯度更加敏感，对不纯度的惩罚最强.但是在实际使用中，信息熵和基尼系数的效果基本相同.但是信息熵的计算比基尼系数缓慢一些，因为基尼系数的计算不涉及对数.另外，因为信息熵对不纯度更加敏感，所以信息熵作为指标时，决策树的生长会更加"精细",因此对于高纬数据或者噪声很多的数据，信息熵很容易过拟合，基尼系数在这种情况下效果往往比较.当模型拟合程度不足时，即当模型在训练集和测试集上都表现不太好的时候，使用信息熵.当然，这些不是绝对的.

{% highlight python linenos %}
from sklearn import tree
from sklearn.datasets import load_wine      # 载入scikit-learn 内置的红酒数据集
from sklearn.model_selection import train_test_split    #数据集拆分工具
wine = load_wine()
print(wine.feature_names)
print(wine.target_names)
Xtrain,Xtest,Ytrain,Ytext = train_test_split(wine.data,wine.target,test_size=0.3)   #拆分训练集和测试集，test_size=0.3表示数据集的百分之三十拿来做测试集
print(Xtrain.shape)
clf = tree.DecisionTreeClassifier(criterion='entropy')
clf = clf.fit(Xtrain,Ytrain)
score = clf.score(Xtest,Ytext)  #返回预测的准确度
print(score)
{% endhighlight %}

#### 1.1.2 如何画出决策树以及判断特征重要性
{% highlight python linenos %}
from sklearn import tree
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
wine = load_wine()
Xtrain,Xtest,Ytrain,Ytest = train_test_split(wine.data,wine.target,test_size=0.25)
clf = tree.DecisionTreeClassifier(criterion='entropy')
clf = clf.fit(Xtrain,Ytrain)
score = clf.score(Xtest,Ytest)
print(score)
import graphviz
dot_data = tree.export_graphviz(clf
                                ,feature_names = wine.feature_names
                                ,class_names = ['0','1','2']
                                ,filled = True      # filled = Ture是给决策树添加颜色，颜色越深不纯度越低；
                                ,rounded = True)    # rounded = Ture是让决策树的方框变圆；每一行逗号写在前面，是为了看这个参数有什么效果时候，方便把其参数注释掉
graph = graphviz.Source(dot_data)
graph.view()
print(clf.feature_importances_)     # 判断决策树的特征重要性
print([*zip(wine.feature_names,clf.feature_importances_)])      #将特征和特征重要性放在一起，方便观察
{% endhighlight %}

#### 1.1.3 random_state或者splitter

random_state用来设置分支中的随机模式的参数；splitter也是用来控制决策树中的随机选项的，有两种输入值，输入‘best’决策树在分支时候虽然随机但是会优先选择更重要的特征进行分支，重要性也是通过feature_importances_查看；输入'random'决策树分支会更加随机，对训练集的拟合将会降低，可以防止过拟合。

{% highlight python linenos %}
clf = tree.DecisionTreeClassifier(criterion='entropy'
                                  ,random_state=30
                                  ,splitter='random')
clf = clf.fit(Xtrain,Ytrain)
score = clf.score(Xtest,Ytest)
print(score)
import graphviz
dot_data = tree.export_graphviz(clf
                                ,feature_names = wine.feature_names
                                ,class_names=['0','1','2']
                                ,filled=True
                                ,rounded=True)
graph = graphviz.Source(dot_data)
print(graph)
graph.view()
{% endhighlight %}

####  1.1.4 剪枝参数调优
1.**max_depth**:限制树的最大深度，超过设定深度的树枝全部剪掉。

这是用的最广泛的剪枝参数，在高纬度低样本量时候非常有效。因为决策树多生长一层，对样本量的需求将增加一倍，所以，限制树的深度能够有效地限制过拟合。实际使用时候，建议从=3开始尝试，看看拟合效果再决定是否增加设定深度。

2.**min_samples_leaf & min_samples_split**

min_samples_leaf是指一个节点再<b>分枝后的每个子节点</b>都必须包含至少min_samples_leaf个训练样本，否则，分枝就不会发生。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。
min_samples_split是指<b>一个节点必须包含</b>至少min_samples_split个训练样本，这个节点才被允许分枝，否则分枝就不会发生。

<b>其实，这两个参数就是为了限定叶子和节点的训练样本个数，因为如果数目太少，就没有必要分枝，防止过拟合。</b>

3.**max_features & min_impurity_decrease**

一般配合max_depth使用，用作树的“精修”。max_features限制分枝时<b>考虑的特征个数</b>，超过限定个数的特征都会被舍弃掉；max_features是用来限制高纬度数据的过拟合的剪枝参数，但是方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数。在不知道决策树中各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。

如果希望通过降维的方式防止过拟合，建议使用PCA、ICA或者特征选择模块中的降维算法。

min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。信息增益是特征选择的一个重要指标，信息增益越大，特征越重要。

{% highlight python linenos %}
from sklearn import tree
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
wine = load_wine()
Xtrain,Xtest,Ytrain,Ytest = train_test_split(wine.data,wine.target,test_size=0.25)
clf = tree.DecisionTreeClassifier(criterion='entropy'
                                  ,random_state=30
                                  ,splitter='random'
                                  ,max_depth=3
                                  ,min_samples_leaf=10  #根据score = clf.score(Xtest,Ytest)大小，决定是否需要这个条件
                                  ,min_samples_split=10
)
clf = clf.fit(Xtrain,Ytrain)
dot_data = tree.export_graphviz(clf
                                ,feature_names=wine.feature_names
                                ,filled=True
                                ,rounded=True

)
import graphviz
graph = graphviz.Source(dot_data)
graph.view()
score = clf.score(Xtest,Ytest)
print(score)
{% endhighlight %}

#### 1.1.5 如何确定最优的剪枝参数

如何确定每个参数具体的填什么值呢?可以使用确定超参数的曲线来进行判断。
对于已经训练好的决策树模型clf,超参数的学习曲线是一条以超参数的取值为横坐标，模型的度量指标为纵坐标的曲线。它可以用来衡量不同超参数取值下模型的表现，在我们之前建好的决策树模型里，我们的度量指标就是score

{% highlight python linenos %}
import matplotlib.pyplot as plt
test = []
for i in range(10):
    clf = tree.DecisionTreeClassifier(max_depth=i+1
                                      ,criterion='gini'
                                      ,random_state=10
                                      ,splitter='random')
    clf = clf.fit(Xtrain,Ytrain)
    score = clf.score(Xtest,Ytest)
    test.append(score)
plt.plot(range(1,11),test,color='red',label='max_depth')
plt.legend()
plt.show()
{% endhighlight %}

### 1.2 重要属性和接口
属性是在模型训练之后，能够调用查看的模型的各种性质。对决策树来说，最重要的是feature_importances_，能够查看各个特征对模型的重要性。

sklearn中许多算法的接口都是相似的，比如说我们之前已经用到的fit和score，
几乎对每个算法都可以使用。除了这两个接口之外，决策树最常用的接口还有apply和predict。apply中输入测试集返回每个测试样本所在的叶子节点的索引，(clf.apply(Xtest))predict输入测试集返回每个测试样本的标签(clf.predict(Xtest))。在这里不得不提的是，所有接口中要求输入X_train和X_test的部分，输入的特征矩阵必须至少是一个二维矩阵。sklearn不接受任何一维矩阵作为特征矩阵被输入.

## 2.回归树 DecisionTreeRegressor

{% highlight python linenos %}
sklearn.DecisionTreeRegressor(criterion='mse',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,presort=False)
{% endhighlight %}

几乎所有参数，属性以及接口都和分类树一样，但是，需要注意的是，在回归树里面没有标签均衡分布的问题，所以没有class_weight这样的参数。

### 2.1 重要参数，属性和接口

#### 2.1.1 criterion
 
回归树衡量分枝质量的指标：（分类树中criterion用来衡量不纯度）

1）输入'mse'使用均方误差mean squared error(MSE),父节点和叶子节点之间的均方误差的差额，被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。

2）输入'friedman_mse'使用费尔德曼均方误差，这种指标针对潜在分枝中的问题改进后的均方误差。

3）输入'mae'使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失。属性中最重要的依然是feature_importances_,接口依然是apply,fit,predict,score。

MSE的本质其实是样本的真实数据和回归结果的差异，在回归树中，MSE不仅是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。<b>在回归中，我们追求的是，MSE越小越好。</b>
回归树的接口score返回的R平方，并不是MSE。由于R平方和MSE定义，可知均方误差恒为正，但是，sklearn当中使用均方误差作为评判标准时，却是计算'负均方误差'。

#### 2.1.2 交叉验证

<b>用来观察模型的稳定性.</b>原理：将数据划分为n份，依次使用用其中的一份作为测试集，其他的n-1份作为训练集，多次计算模型的精确度来评估模型的平均准确程度。训练集和测试集的划分会干扰模型的结果，因此，用交叉验证n次的结果求出的平均值，是对模型效果的一个更好的度量。

{% highlight python linenos %}
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor  #导入库和数据集，而且回归需要处理连续性数据模型

boston = load_boston()
regressor = DecisionTreeRegressor(random_state=0)   #实例化
a = cross_val_score(regressor,boston.data,boston.target,cv=10
                    ,scoring='neg_mean_squared_error'
                                                    )
print(a) #交叉验证，第一个参数是实例化后的模型；第二个参数完整数据和完整标签；cv=10，交叉验证十次；scoring='neg_mean_squared_error',如果不加这个参数表示用R平方来评估模型,加上这个参数代表用负的均方误差的值评估模型
{% endhighlight %}
